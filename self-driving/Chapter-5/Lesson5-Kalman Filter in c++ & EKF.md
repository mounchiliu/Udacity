## Lesson 5 Kalman Filter in c++ & EKF
- You have already known how apply Kalman filter to estimation problems. But only use it to combine sequential measurements from **one sensor**.

- Actually, it's also possible to use Kalman filters to combine measurements from different sensors, and that's why we use it in sensor fusion

- **EKF** is able to handling more complex **motion models and measurement models**.
![image](https://user-images.githubusercontent.com/47606318/124294318-1d641480-db8a-11eb-8c18-346cce02f34d.png)

### Flowchart of Kalman Filter for fusion with lidar and radar
![image](https://user-images.githubusercontent.com/47606318/124385969-90949480-dd0a-11eb-8e85-d60abce5ae2e.png)

- you have two sensors, a lidar and a radar
- the information provided by these sensors is used to estimate the state of a moving pedestrian
  - this state is represented by a 2D position and a 2D velocity
- each time we receive new measurements from a given sensor, the estimation function is triggered
  - at this point we perform two steps, State Prediction and Measurement Update
  - in the prediction step, we predict the pedestrian state and its covariance
    - we do so by taking into account the elapsed time between the current and the previous observations
- the measurement update step depends on sensor type (**use the new observations to correct our belief about the state of the pedestrian**)
  - there are two cases that we'll see, Laser and Radar
  - if the current measurements are generated by a laser sensor, then we just apply a standard Kalman filter to update the pedestrian's state
  - however, radar measurements involve a nonlinear measurement function
    - so when we receive radar measurements, we use different tweaks to handle the measurement update
    - for instance, we may use Extended Kalman filter equations, which you'll see later



**Q:** but what happens when there are two sensors that observe the same pedestrian? how does that change the Kalman filter?
**ans:**
- actually, we can keep the same processing flow with the difference that each sensor is going to have its **own prediction/update scheme**
- (当不同传感器 measurement 获取不在同一时刻时， update asynchronously, 只是不同传感器的measurement update 不一样)in other words, the belief about the pedestrian's position and velocity is **updated asynchronously** each time new measurement is received **regardless the source sensor**

![image](https://user-images.githubusercontent.com/47606318/124387378-f5eb8400-dd10-11eb-9039-597533e3beec.png)

**Example**

![image](https://user-images.githubusercontent.com/47606318/124387412-2501f580-dd11-11eb-8d68-1931be3cac59.png)

- here the pedestrian state at time  k  is a distribution with its mean  x  and covariance  P 
- let's say we're now at time  k+1 , and we've just received the laser measurement
  - the first thing we do before we look at the measurement update is to make a prediction about where we think the pedestrian from time  k  will be at time  k+1 
  - the second thing we do is the so called measurement update, where we combine the pedestrian's predicted state with the new laser measurement
  - what we now have is a more accurate belief about the pedestrian's position at time  k+1 ; this is what we call the posterior
- now let's imagine that we receive the radar measurement at time  k+2 
  - first, we again predict the pedestrian state from  k+1  to  k+2 
    - note, this prediction for radar is exactly the same function as in the laser case
  - what changes, in this case, is the Measurement Update step
    - as we know, the radar sees the word differently than laser
    - while laser provides measurement in a Cartesian coordinate system, radar provides measurement in a polar coordinate system
    - thus we have to use different measurement update functions specific to radar data, so this is a more detailed view of the Kalman filter
- **we received the measurements from different sensors at each timestamp, and then we make a prediction followed by a measurement update**

![image](https://user-images.githubusercontent.com/47606318/124388009-5085df80-dd13-11eb-8ebb-3d552a4a2c27.png)

**Q:** What should a Kalman Filter do if both the radar and laser measurements arrive at the same time, k+3 ? 

**Hint:** The Kalman filter algorithm predicts -> updates -> predicts -> updates, etc. If two sensor measurements come in simultaneously, the time step between the first measurement and the second measurement would be zero.

**ans:** 
- **Predict** the state to k+3 then use either one of the sensors to update. Then predict the state to k+3 again and update with the other sensor measurement.
- as you saw, the Kalman filter is a two-step process: predict, and then update
- if you receive two measurements simultaneously, you can use this process with either measurement and then repeat the process with the other measurement
  - the order does not matter!
- because we have already run a prediction-update iteration with the first sensor at time  k+3 , **the output of the second prediction at time  k+3  will actually be identical** to the output from the update step with the first sensor
- so, **in theory**, you could **skip the second prediction step** and just run a prediction, update, update iteration

### Kalman filter in c++
The Kalman equation contains many variables, so here is a high level overview to get some intuition about what the Kalman filter is doing.

#### Intuition

**Prediction**
- Let's say we know an object's current position and velocity , which we keep in the x variable. 
- Now one second has passed. We can predict where the object will be one second later because we knew the object position and velocity one second ago; we'll just assume the object kept going at the same velocity.
- The x′=Fx+ν equation does these prediction calculations for us.

- But maybe the object **didn't maintain the exact same velocity**. Maybe the object changed direction, accelerated or decelerated. 
- So when we predict the position one second later, our uncertainty increases. 
- Update uncertainty: P′=FPF^T+Q represents this increase in uncertainty.

- Process noise refers to the uncertainty in the prediction step. 
- We assume the object travels at a constant velocity, but in reality, the object might accelerate or decelerate. 
- The notation ν∼N(0,Q) defines the process noise as a gaussian distribution with mean zero and covariance Q.

**Measuremtn Update**
- Now we get some sensor information that tells where the object is relative to the car. 
- First we compare where we think we are with what the sensor data tells us y=z−Hx′.

- The K matrix, often called the Kalman filter gain, combines the uncertainty of where we think we are P′ with the uncertainty of our sensor measurement R. 
- If our sensor measurements are very uncertain (R is high relative to P'), then the Kalman filter will give more weight to where we think we are: x′
- If where we think we are is uncertain (P' is high relative to R), the Kalman filter will put more weight on the sensor measurement: z.

- Measurement noise refers to uncertainty in sensor measurements. 
- The notationω∼N(0,R) defines the measurement noise as a gaussian distribution with mean zero and covariance R.


**A Note About the State Transition Function: Bu**
- if you go back to the video, you'll notice that the state transition function was first given as **x′=Fx+Bu+ν** 
- But then  Bu  was crossed out leaving  x′=Fx+ν 

- B is a matrix called the control input matrix and  u is the control vector
- as an example, let's say we were tracking a car and we knew for certain how much the car's motor was going to accelerate or decelerate over time; in other words, we had an equation to model the exact amount of acceleration at any given moment
- Bu would represent the updated position of the car due to the internal force of the motor
- we would use  ν  to represent any random noise that we could not precisely predict like if the car slipped on the road or a strong wind moved the car


- for the Kalman filter lessons, we will **assume that there is no way to measure or know the exact acceleration** of a tracked object
- for example, if we were in an autonomous vehicle tracking a bicycle, pedestrian or another car, we would **not be able to model the internal forces** of the other object; hence, we do not know for certain what the other object's acceleration is
- instead, we will set  Bu=0  and represent acceleration as a random noise with mean  ν

#### Equations in C++
![image](https://user-images.githubusercontent.com/47606318/124488782-d1f87300-dde2-11eb-89c6-693413f0052c.png)

![image](https://user-images.githubusercontent.com/47606318/124488819-dde43500-dde2-11eb-8cbd-557f307a71db.png)

![image](https://user-images.githubusercontent.com/47606318/124488905-fb190380-dde2-11eb-9009-ad756cf70b7b.png)

Notice:
- Over time, the order of these doesn't have a huge impact, since it is just a cycle from one to the other.
- Here, the first thing you need is a measurement because otherwise there is no location information or even information that the object exists unless a sensor picked it up. 
- So, you initialize location values with the measurement.

#### 1D tracking problem implementation in c++

```cpp
/** 
 * Write a function 'filter()' that implements a multi-
 *   dimensional Kalman Filter for the example given
 */

#include <iostream>
#include <vector>
#include "Dense"

using std::cout;
using std::endl;
using std::vector;
using Eigen::VectorXd;
using Eigen::MatrixXd;

// Kalman Filter variables
VectorXd x;	// object state
MatrixXd P;	// object covariance matrix
VectorXd u;	// external motion
MatrixXd F; // state transition matrix
MatrixXd H;	// measurement matrix
MatrixXd R;	// measurement covariance matrix
MatrixXd I; // Identity matrix
MatrixXd Q;	// process covariance matrix

vector<VectorXd> measurements;
void filter(VectorXd &x, MatrixXd &P);


int main() {
  /**
   * Code used as example to work with Eigen matrices
   */
  // design the KF with 1D motion
  x = VectorXd(2);
  x << 0, 0;

  P = MatrixXd(2, 2);
  P << 1000, 0, 0, 1000;

  u = VectorXd(2);
  u << 0, 0;

  F = MatrixXd(2, 2);
  F << 1, 1, 0, 1;

  H = MatrixXd(1, 2);
  H << 1, 0;

  R = MatrixXd(1, 1);
  R << 1;

  I = MatrixXd::Identity(2, 2);

  Q = MatrixXd(2, 2);
  Q << 0, 0, 0, 0;

  // create a list of measurements
  VectorXd single_meas(1);
  single_meas << 1;
  measurements.push_back(single_meas);
  single_meas << 2;
  measurements.push_back(single_meas);
  single_meas << 3;
  measurements.push_back(single_meas);

  // call Kalman filter algorithm
  filter(x, P);

  return 0;
}
  
void filter(VectorXd &x, MatrixXd &P) {

  for (unsigned int n = 0; n < measurements.size(); ++n) {

    VectorXd z = measurements[n];
    // TODO: YOUR CODE HERE
    /**
     * KF Measurement update step
     */
    VectorXd y = z - H * x;
    MatrixXd Ht = H.transpose();
    MatrixXd S = H * P * Ht + R;
    MatrixXd Si = S.inverse();
    MatrixXd K =  P * Ht * Si;

    // new state
    x = x + (K * y);
    P = (I - K * H) * P;

    /**
     * KF Prediction step
     */
    x = F * x + u;
    MatrixXd Ft = F.transpose();
    P = F * P * Ft + Q;

    cout << "x=" << endl <<  x << endl;
    cout << "P=" << endl <<  P << endl;
  }
}
```
    
    
**Q:**

Why do we not use the process noise in the state prediction function, even though the state transition equation has one? In other words, why does the code set u << 0, 0 for the equation  x=F∗x+u ?

**Ans：**

![image](https://user-images.githubusercontent.com/47606318/124614522-e69e3f00-dea6-11eb-8374-a2b7678680e0.png)

- Because the noise mean is zero. 
- Looking closely at the process noise, we know from the Kalman Filter algorithm that its mean is zero and its covariance matrix is usually noted by N(0,Q)  The first equation only predicts the mean state. As the mean value of the noise is zero, it does not directly affect the predicted state. However, we can see that the noise covariance  Q  is added here to the state covariance prediction so that the state uncertainty always increases through the process noise


### 2D tracking problem with Kalman Filter
- as our pedestrian is moving along both horizontal and vertical directions

  we now want to estimate a 2D position and 2D velocity
  
![image](https://user-images.githubusercontent.com/47606318/124616622-d38c6e80-dea8-11eb-9f19-c96f3af9143f.png)

- second, we'll use the same linear motion model with the custom velocity
  - so, the new  x  and  y  position will be the old positions + the displacement, which is the same as the velocity * delta t
     - p′x=px+vxΔt+νpx(noise)
     - p′y=py+vyΔt+νpy(noise)
  - the velocity along both  x  and  y  axes stays the same
     - v′x=vx+νvx(noise)
     - v′y=vy+νvy(noise)
   - these are kinematic formulas
![image](https://user-images.githubusercontent.com/47606318/124617467-88269000-dea9-11eb-9e45-6fe5fc4c5c12.png)

**Notice：**
- Δt  is not constant anymore
- in reality, the time elapsed between two consecutive observations might vary

**Remainder: motion noice (process noise) VS measurement noise**
- Motion noise and process noise refer to the same case: uncertainty in the object's position when predicting location. The model **assumes velocity is constant between time intervals**, but **in reality** we know that an **object's velocity can change due to acceleration**. The model includes this **uncertainty** via the **process noise**.

- **Measurement noise** refers to **uncertainty** in sensor measurements, which will be discussed in more detail later.

**Q1:** Suppose you have a pedestrian state X. I want you to compare two scenarios: in the first predict the state 0.1s into the future and in the second 5s into the future. Which of these two scenarios leads to a higher uncertainty? In answering this, consider whether or not random noise has an increasing effect with increasing gaps between prediction times.

**A:** A time difference of 5s leads to a higher uncertainty. Here's another way of thinking about it: if you split the 5s time difference into several intermediate predictions with a difference of 0.1s, then compared to the first case, you will predict the same state many more times without receiving any feedback about the object's new position. Thus, the uncertainty increases.

**Q2:** Let's say we use our linear motion model with fixed time increments, but the pedestrian is randomly changing her velocity (accelerating), sometimes speeding up, slowing down or changing direction. However, the overall mean change is zero. This introduces a noise in the tracking process - what kind of noise is it?

**A:** It's process noise. The prediction equation treats the pedestrian's velocity as constant. As such, a randomly accelerating pedestrian creates a process noise.

**We can clearly see that the process noise depends on both: the elapsed time and the uncertainty of acceleration**

#### Process Covariance Matrix
##### Calculating Acceleration Noise Parameters
- before we discuss the derivation of the process covariance matrix $Q$ (e.g. Q ~ N(0,  $\sigma^2$), you might be curious about how to choose values for $\sigma_{ax}^2$ and $\sigma_{ay}^2$

#### Process Covariance Matrix Q - Intuition
- as a reminder, here are the state covariance matrix update equation and the equation for $Q$
- $P' = FPF^T + Q$


- $Q = \begin{pmatrix} \frac{\Delta t^4}{{4}}\sigma_{ax}^2 & 0 & \frac{\Delta t^3}{{2}}\sigma_{ax}^2 & 0 \\ 0 & \frac{\Delta t^4}{{4}}\sigma_{ay}^2 & 0 & \frac{\Delta t^3}{{2}}\sigma_{ay}^2 \\ \frac{\Delta t^3}{{2}}\sigma_{ax}^2 & 0 & \Delta t^2\sigma_{ax}^2 & 0 \\ 0 & \frac{\Delta t^3}{{2}}\sigma_{ay}^2 & 0 & \Delta t^2\sigma_{ay}^2 \end{pmatrix}$


- because our state vector only **tracks position and velocity**, we are **modeling acceleration as a random noise**
- the $Q$ matrix **includes time $\Delta t$ to account** for the fact that **as more time passes, we become more uncertain** about our position and velocity
  - so as $\Delta t$ increases, we add more uncertainty to the state covariance matrix $P$


- say we have two consecutive observations of the same pedestrian with initial and final velocities
- from the kinematic formulas, we can derive the current position and speed as a function of previous state variables, including the change in the velocity, or in other words, including the acceleration $a = \dfrac{\Delta v}{\Delta t} = \dfrac{v_{k+1} - v_k}{\Delta t}$


- combining both 2D position and 2D velocity equations previously deducted formulas we have:
$\begin{cases} p_x' = p_x + v_x \Delta t + \frac{a_x \Delta t^2}{{2}}\\ p_y' = p_y + v_y \Delta t + \frac{a_y \Delta t^2}{{2}}\\ v_x' = v_x + a_x \Delta t\\ v_y' = v_y + a_y \Delta t \end{cases}$
  - looking at the deterministic part of our motion model, we assume that the velocity is constant
    - however, in reality, the pedestrian speed might change

- since the acceleration is unknown, we can add it to the noise component
    - this random noise would be expressed analytically as in the last terms in the equation
- so, we have a random acceleration vector $\nu$ in this form: $\nu = \begin{pmatrix} \nu_{px} \\ \nu_{py} \\ \nu_{vx} \\ \nu_{vy} \end{pmatrix} = \begin{pmatrix} \frac{a_x \Delta t^2}{{2}} \\ \frac{a_y \Delta t^2}{{2}} \\ a_x \Delta t \\ a_y \Delta t \end{pmatrix}$ which is described by a zero mean and a covariance matrix $Q$, so $\nu \sim N(0,Q)$


- this vector $\nu$ can be decomposed into two components: a 4 by 2 matrix $G$ which does not contain random variables and a 2 by 1 matrix $a$ which contains the random acceleration components: $\nu = \begin{pmatrix} \frac{a_x \Delta t^2}{{2}} \\ \frac{a_y \Delta t^2}{{2}} \\ a_x \Delta t \\ a_y \Delta t \end{pmatrix} = \begin{pmatrix} \frac{\Delta t^2}{{2}} & 0 \\ 0 & \frac{\Delta t^2}{{2}} \\ \Delta t & 0 \\ 0 & \Delta t \end{pmatrix} \begin{pmatrix} a_x\\ a_y \end{pmatrix} = Ga$


- $\Delta t$ is computed at each Kalman filter step and the acceleration is a random vector with zero mean and standard deviations, $\sigma_{ax}^2$ and $\sigma_{ay}^2$
  - $a_x \sim N(0,\sigma_{ax}^2)$
  - $a_y \sim N(0,\sigma_{ay}^2)$

- based on our noise vector we can define now the new covariance matrix $Q$
- the covariance matrix is defined as the expectation value of the noise vector $\nu$ times the noise vector $\nu^T$
  - $Q = E[\nu \nu^T] = E[Gaa^TG^T]$

- as matrix $G$ does not contain random variables, we can put it outside the expectation calculation
  - $Q = G E[aa^T] G^T = G \begin{pmatrix} \sigma_{ax}^2 & \sigma_{axy} \\ \sigma_{axy} & \sigma_{ay}^2 \end{pmatrix} G^T = G Q_{\nu} G^T$
- this leaves us with three statistical moments:
  - the expectation of ax times ax, which is the variance of ax squared: $\sigma_{ax}^2$
  - the expectation of ay times ay, which is the variance of ay squared: $\sigma_{ay}^2$
  - the expectation of ax times ay, which is the covariance of ax and ay: $\sigma_{axy}$


- $a_x$ and $a_y$ are assumed uncorrelated noise processes
  - this means that the covariance $\sigma_{axy}$ in $Q_{\nu}$ is zero: $Q_{\nu} = \begin{pmatrix} \sigma_{ax}^2 & \sigma_{axy} \\ \sigma_{axy} & \sigma_{ay}^2 \end{pmatrix} = \begin{pmatrix} \sigma_{ax}^2 & 0 \\ 0 & \sigma_{ay}^2 \end{pmatrix}$


- so after combining everything in one matrix we obtain our 4 by 4 $Q$ matrix: $Q = G Q_{\nu} G^T = \begin{pmatrix} \frac{\Delta t^4}{{4}}\sigma_{ax}^2 & 0 & \frac{\Delta t^3}{{2}}\sigma_{ax}^2 & 0 \\ 0 & \frac{\Delta t^4}{{4}}\sigma_{ay}^2 & 0 & \frac{\Delta t^3}{{2}}\sigma_{ay}^2 \\ \frac{\Delta t^3}{{2}}\sigma_{ax}^2 & 0 & \Delta t^2\sigma_{ax}^2 & 0 \\ 0 & \frac{\Delta t^3}{{2}}\sigma_{ay}^2 & 0 & \Delta t^2\sigma_{ay}^2 \end{pmatrix}$

